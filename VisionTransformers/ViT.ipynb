{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuc9o2/NzxZdSTU6sUKQDH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilakshanKunananthaseelan/research/blob/main/VisionTransformers/ViT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPdQu91ZDbY4"
      },
      "source": [
        "%%bash\n",
        "pip install einops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAjb-W-cDXOE"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn. functional as F\n",
        "\n",
        "import PIL \n",
        "import torchvision\n",
        "\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "\t\"\"\"A specialized dropout layer for use with Monte Carlo dropout inference.\n",
        "  Can operates exactly as tf.keras.layers.Dropout during training or inference, \n",
        "  but has the option to generate and store a collection of masks for \n",
        "  reproducible Monte Carlo dropout inference (see 'fix_masks'). Requires \n",
        "  output_shape to be set as when used in a tf.keras.Model.\n",
        "  \n",
        "  Attributes\n",
        "  ----------\n",
        "  self.fixed : Bool\n",
        "    Indicates whether or not the layers have been fixed\n",
        "  self.masks : array_like\n",
        "    A length x output_shape \n",
        "  Methods\n",
        "  -------\n",
        "  call\n",
        "    Can be called like a standard dropout layer, or optionally with with a \n",
        "    'sample' parameter to select which of the fixed masks to apply.\n",
        "  \n",
        "  fix_masks\n",
        "    Generates a sequence of 'length' masks and stores them in the layer for \n",
        "    future re-use. Sets 'fixed' attribute to true.\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef __init__(self,normalize_dim,function):\n",
        "\t\tsuper().__init__()\n",
        "\n",
        "\t\tself.layer_norm = nn.LayerNorm(normalize_dim)\n",
        "\t\tself.function   = function\n",
        "\n",
        "\tdef forward(self,x,**kwargs):\n",
        "\t\tx = self.layer_norm(x)\n",
        "\t\tx = self.function(x,**kwargs)\n",
        "\n",
        "\t\treturn x\n",
        "\n",
        "\n",
        "\n",
        "class ResidualConnect(nn.Module):\n",
        "\n",
        "\tdef __init__(self,function):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.function = function\n",
        "\n",
        "\tdef forward(self,x,**kwargs):\n",
        "\t\tx = self.function(x,**kwargs)+x\n",
        "\t\treturn x\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRB6777hXv57"
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn \n",
        "\n",
        "\n",
        "class MLP_Block(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,hidden_dim,dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.fc1 = nn.Linear(input_dim,hidden_dim,bias=True)\n",
        "    self.gelu1 = nn.GELU()\n",
        "    self.dropou1 = nn.Dropout(dropout)\n",
        "\n",
        "    self.fc2 = nn.Linear(hidden_dim,input_dim,bias=True)\n",
        "    self.gelu2 = nn.GELU()\n",
        "    self.dropou2 = nn.Dropout(dropout)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "\n",
        "\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    self.fc1.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "    torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
        "    self.fc2.bias.data.fill_(0.01)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.gelu1(x)\n",
        "    x = self.dropou1(x)\n",
        "\n",
        "    x = self.fc2(x)\n",
        "    x = self.gelu1(x)\n",
        "    x = self.dropou2(x)\n",
        "\n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORRGoVbIDhOE"
      },
      "source": [
        "\n",
        "#https://jalammar.github.io/illustrated-transformer/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn. functional as F\n",
        "\n",
        "import PIL \n",
        "import torchvision\n",
        "\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,heads=8,dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.heads = heads\n",
        "    self.scale = input_dim**0.5\n",
        "\n",
        "    self.mat_calc = nn.Linear(input_dim,input_dim*3)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    self.fc1 = nn.Linear(input_dim,input_dim,bias=True)\n",
        "    self.dropou1 = nn.Dropout(dropout)\n",
        "\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def init_weights(self):\n",
        "\n",
        "\n",
        "    torch.nn.init.xavier_uniform_(self.mat_calc.weight)\n",
        "    self.mat_calc.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    self.fc1.bias.data.fill_(0.01)\n",
        "\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "\n",
        "    b, n, _= x.shape\n",
        "    h = self.heads\n",
        "    qkv = self.mat_calc(x)\n",
        "\n",
        "    q,k,v = rearrange(qkv,'b n (w h d) -> w b h n d',w=3,h=8)/self.scale\n",
        "\n",
        "    dot_product = torch.einsum('bhid,bhjd->bhij',q,k)\n",
        "\n",
        "    if mask is not None:\n",
        "\n",
        "      mask = F.pad(mask.flatten(1), (1, 0), value = True)\n",
        "      assert mask.shape[-1] == dot_product.shape[-1], 'mask has incorrect dimensions'\n",
        "      mask = mask[:, None, :] * mask[:, :, None]\n",
        "      dot_product.masked_fill_(~mask, float('-inf'))\n",
        "      del mask\n",
        "\n",
        "\n",
        "\n",
        "    softmax_out = self.softmax(dot_product)\n",
        "\n",
        "    attention_matrices = torch.einsum('bhij,bhjd->bhid',softmax_out,v)\n",
        "\n",
        "    out = rearrange(attention_matrices,'b h n d->b n (h d)')\n",
        "    out = self.fc1(out)\n",
        "    out = self.dropou1(out)\n",
        "    return out\n",
        "\n",
        "class EncoderLayers(nn.Module):\n",
        "  def __init__(self,input_dim,heads,mlp_dim,dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention_block = ResidualConnect(LayerNorm(input_dim,Attention(input_dim,heads=heads,dropout=dropout))) \n",
        "    self.mlp_block = ResidualConnect(LayerNorm(input_dim,MLP_Block(input_dim,mlp_dim,dropout=dropout)))\n",
        "\n",
        "\n",
        "  # def forward(self,x,mask=None):\n",
        "  #   x = self.attention_block(x,mask=mask)\n",
        "  #   x = self.mlp_block(x)\n",
        "\n",
        "  #   return x\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,depth,heads,mlp_dim,dropout=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_layers = nn.ModuleList([])\n",
        "\n",
        "    for _ in range(depth):\n",
        "      self.encoder_layers.append(nn.ModuleList([ResidualConnect(LayerNorm(input_dim,Attention(input_dim,heads=heads,dropout=dropout))),\n",
        "      ResidualConnect(LayerNorm(input_dim,MLP_Block(input_dim,mlp_dim,dropout=dropout)))]))\n",
        "\n",
        "  def forward(self,x,mask=None):\n",
        "      for attention_block,mlp_block in self.encoder_layers:\n",
        "\n",
        "        x = attention_block(x,mask=mask)\n",
        "        x = mlp_block(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPKCwa-QgHVe"
      },
      "source": [
        "t = TransformerEncoder(64,1,8,128)\n",
        "x = torch.randint(0,1,(1,64,64)).float()\n",
        "def xsqr(x):\n",
        "  return x*x\n",
        "\n",
        "r = ResidualConnect(xsqr)\n",
        "l = LayerNorm(1,xsqr)\n",
        "l(torch.tensor(5).reshape(-1,1\n",
        "                          ).float())\n",
        "\n",
        "t(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81mUQEs-Dx44"
      },
      "source": [
        "  import torch\n",
        "  import torch.nn as nn\n",
        "\n",
        "  from einops import rearrange, repeat\n",
        " \n",
        "\n",
        "  class ViT(nn.Module):\n",
        "    def __init__(self,*,image_size,patch_size,num_classes,input_dim,depth,heads,mlp_dim,channels=3,dropout=0.1,embedding_dropout=0.1):\n",
        "      super().__init__()\n",
        "\n",
        "\n",
        "      assert not(image_size%patch_size),'image dimension should be factor of patch dimension'\n",
        "\n",
        "      num_patches = (image_size//patch_size)**2\n",
        "\n",
        "      patches_dim = channels*patch_size**2\n",
        "\n",
        "      self.patch_size = patch_size\n",
        "      self.pos_embeds =  nn.Parameter(torch.empty(1, (num_patches + 1), input_dim))\n",
        "\n",
        "      self.patch_conv = nn.Conv2d(3,input_dim,kernel_size=patch_size,stride=patch_size)#change input_dim name\n",
        "\n",
        "\n",
        "      self.cls_embeds = nn.Parameter(torch.zeros(1,1,input_dim))\n",
        "      self.dropout = nn.Dropout(embedding_dropout)\n",
        "\n",
        "      self.transformer_encoder = TransformerEncoder(input_dim,depth,heads,mlp_dim,dropout)\n",
        "\n",
        "      self.to_cls_embeds =  nn.Identity()\n",
        "\n",
        "      self.mlp_head = nn.Linear(input_dim,num_classes)\n",
        "\n",
        "      self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "\n",
        "      torch.nn.init.normal_(self.pos_embeds, std = .02) # initialized based on the paper\n",
        "\n",
        "      torch.nn.init.xavier_uniform_(self.mlp_head.weight)\n",
        "      torch.nn.init.normal_(self.mlp_head.bias, std = 1e-6)\n",
        "\n",
        "\n",
        "    def forward(self,img,mask=None):\n",
        "\n",
        "\n",
        "      img_patches = self.patch_conv(img)\n",
        "\n",
        "      x = rearrange(img_patches,'b c h w -> b (h w) c')\n",
        "      cls_embeds = self.cls_embeds.expand(img.shape[0],-1,-1)\n",
        "\n",
        "      x = torch.cat((cls_embeds,x),dim=1)\n",
        "      x+= self.pos_embeds\n",
        "      x = self.dropout(x)\n",
        "      x = self.transformer_encoder(x,mask)\n",
        "      x = self.to_cls_embeds(x[:,0])\n",
        "\n",
        "      x = self.mlp_head(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP2WGdzkYAZN"
      },
      "source": [
        "BATCH_SIZE_TRAIN = 500\n",
        "BATCH_SIZE_TEST = 100\n",
        "\n",
        "DL_PATH = \"C:\\Pytorch\\Spyder\\CIFAR10_data\" # Use your own path\n",
        "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
        "transform = torchvision.transforms.Compose(\n",
        "     [torchvision.transforms.RandomHorizontalFlip(),\n",
        "     torchvision.transforms.RandomRotation(10, resample=PIL.Image.BILINEAR),\n",
        "     torchvision.transforms.RandomAffine(8, translate=(.15,.15)),\n",
        "     torchvision.transforms.ToTensor(),\n",
        "     torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(DL_PATH, train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(DL_PATH, train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE_TRAIN,\n",
        "                                          shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE_TEST,\n",
        "                                         shuffle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEJcQIQMYOK8"
      },
      "source": [
        "import time\n",
        "def train(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        data = data.to('cuda')\n",
        "        target = target.to('cuda')\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())\n",
        "            \n",
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "    \n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            data = data.to('cuda')\n",
        "            target = target.to('cuda')\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8CSTXo-YRRL"
      },
      "source": [
        "N_EPOCHS = 150\n",
        "\n",
        "model = ViT(image_size=32, patch_size=4, num_classes=10, channels=3,\n",
        "            input_dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "model = model.to('cuda')\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    start_time = time.time()\n",
        "    train(model, optimizer, train_loader, train_loss_history)\n",
        "    print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time')\n",
        "\n",
        "PATH = \"ViTnet_Cifar10_4x4_aug_1.pt\" # Use your own path\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJNuSrCHoK32"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}