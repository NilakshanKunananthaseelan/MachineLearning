{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WeighDecay.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNUdhQphW9es7Am5zuSkcbD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NilakshanKunananthaseelan/MachineLearning/blob/main/WeighDecay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUz2Yqx01fFA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "#synthetic data generator\n",
        "def synthetic_data(w, b, num_examples):\n",
        "    \"\"\"generate y = X w + b + noise\"\"\"\n",
        "    X = np.random.normal(scale=1, size=(num_examples, len(w)))\n",
        "    y = np.dot(X, w) + b\n",
        "    y += np.random.normal(scale=0.01, size=y.shape)\n",
        "    y = np.where(y<0.15,1,0)\n",
        "    return X, y\n",
        "    \n",
        "#set the number of samples for training,testing\n",
        "n_train, n_test, num_inputs, batch_size = 20, 1000, 10, 5\n",
        "true_w, true_b = torch.ones((num_inputs, 1)) * 0.01, 0.05\n",
        "train_X,train_y = synthetic_data(true_w,true_b,n_train)\n",
        "test_X,test_y = synthetic_data(true_w,true_b,n_test)\n",
        "\n",
        "# create the Pytorch dataset and dataloader\n",
        "train_X = torch.Tensor(train_X)\n",
        "train_y = torch.Tensor(train_y)\n",
        "test_X = torch.Tensor(test_X)\n",
        "test_y = torch.Tensor(test_y)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(train_X,train_y)\n",
        "train_loader = DataLoader(train_dataset)\n",
        "\n",
        "test_dataset = TensorDataset(test_X,test_y)\n",
        "test_loader = DataLoader(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(10, 100)\n",
        "        self.fc2 = nn.Linear(100, 2)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "Eh72ImdT1vwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loader),len(test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYPXmDpW_3xy",
        "outputId": "376b9ab1-2ca9-4fc3-dfd2-a7aef432a68b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "net = Net()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.0,weight_decay=0.000)\n",
        "\n",
        "train_loss = []\n",
        "\n",
        "nepoch = 50\n",
        "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "  \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if i % 5 == 1:    # print every 200 mini-batches\n",
        "            print(f'[{epoch + 1}/{nepoch}] loss: {running_loss / 200:.6f}')\n",
        "            running_loss = 0.0\n",
        "    train_loss.append(running_loss)\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyuWkH5s14O_",
        "outputId": "680baba0-4b9a-4a22-d149-595e67d1cc82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50] loss: 0.009803\n",
            "[1/50] loss: 0.030446\n",
            "[1/50] loss: 0.025902\n",
            "[1/50] loss: 0.023906\n",
            "[2/50] loss: 0.007683\n",
            "[2/50] loss: 0.023441\n",
            "[2/50] loss: 0.017772\n",
            "[2/50] loss: 0.016596\n",
            "[3/50] loss: 0.006131\n",
            "[3/50] loss: 0.018287\n",
            "[3/50] loss: 0.012308\n",
            "[3/50] loss: 0.011591\n",
            "[4/50] loss: 0.004985\n",
            "[4/50] loss: 0.014455\n",
            "[4/50] loss: 0.008651\n",
            "[4/50] loss: 0.008167\n",
            "[5/50] loss: 0.004133\n",
            "[5/50] loss: 0.011580\n",
            "[5/50] loss: 0.006219\n",
            "[5/50] loss: 0.005829\n",
            "[6/50] loss: 0.003494\n",
            "[6/50] loss: 0.009402\n",
            "[6/50] loss: 0.004616\n",
            "[6/50] loss: 0.004236\n",
            "[7/50] loss: 0.003011\n",
            "[7/50] loss: 0.007735\n",
            "[7/50] loss: 0.003570\n",
            "[7/50] loss: 0.003152\n",
            "[8/50] loss: 0.002642\n",
            "[8/50] loss: 0.006446\n",
            "[8/50] loss: 0.002893\n",
            "[8/50] loss: 0.002415\n",
            "[9/50] loss: 0.002358\n",
            "[9/50] loss: 0.005437\n",
            "[9/50] loss: 0.002458\n",
            "[9/50] loss: 0.001914\n",
            "[10/50] loss: 0.002136\n",
            "[10/50] loss: 0.004639\n",
            "[10/50] loss: 0.002181\n",
            "[10/50] loss: 0.001571\n",
            "[11/50] loss: 0.001962\n",
            "[11/50] loss: 0.004000\n",
            "[11/50] loss: 0.002004\n",
            "[11/50] loss: 0.001336\n",
            "[12/50] loss: 0.001822\n",
            "[12/50] loss: 0.003482\n",
            "[12/50] loss: 0.001890\n",
            "[12/50] loss: 0.001173\n",
            "[13/50] loss: 0.001710\n",
            "[13/50] loss: 0.003058\n",
            "[13/50] loss: 0.001813\n",
            "[13/50] loss: 0.001058\n",
            "[14/50] loss: 0.001617\n",
            "[14/50] loss: 0.002707\n",
            "[14/50] loss: 0.001758\n",
            "[14/50] loss: 0.000976\n",
            "[15/50] loss: 0.001541\n",
            "[15/50] loss: 0.002413\n",
            "[15/50] loss: 0.001716\n",
            "[15/50] loss: 0.000915\n",
            "[16/50] loss: 0.001476\n",
            "[16/50] loss: 0.002164\n",
            "[16/50] loss: 0.001679\n",
            "[16/50] loss: 0.000869\n",
            "[17/50] loss: 0.001422\n",
            "[17/50] loss: 0.001952\n",
            "[17/50] loss: 0.001646\n",
            "[17/50] loss: 0.000832\n",
            "[18/50] loss: 0.001374\n",
            "[18/50] loss: 0.001770\n",
            "[18/50] loss: 0.001613\n",
            "[18/50] loss: 0.000803\n",
            "[19/50] loss: 0.001333\n",
            "[19/50] loss: 0.001613\n",
            "[19/50] loss: 0.001580\n",
            "[19/50] loss: 0.000778\n",
            "[20/50] loss: 0.001297\n",
            "[20/50] loss: 0.001475\n",
            "[20/50] loss: 0.001546\n",
            "[20/50] loss: 0.000756\n",
            "[21/50] loss: 0.001265\n",
            "[21/50] loss: 0.001355\n",
            "[21/50] loss: 0.001512\n",
            "[21/50] loss: 0.000737\n",
            "[22/50] loss: 0.001236\n",
            "[22/50] loss: 0.001249\n",
            "[22/50] loss: 0.001477\n",
            "[22/50] loss: 0.000720\n",
            "[23/50] loss: 0.001209\n",
            "[23/50] loss: 0.001155\n",
            "[23/50] loss: 0.001442\n",
            "[23/50] loss: 0.000705\n",
            "[24/50] loss: 0.001185\n",
            "[24/50] loss: 0.001072\n",
            "[24/50] loss: 0.001407\n",
            "[24/50] loss: 0.000690\n",
            "[25/50] loss: 0.001163\n",
            "[25/50] loss: 0.000998\n",
            "[25/50] loss: 0.001372\n",
            "[25/50] loss: 0.000677\n",
            "[26/50] loss: 0.001142\n",
            "[26/50] loss: 0.000932\n",
            "[26/50] loss: 0.001338\n",
            "[26/50] loss: 0.000665\n",
            "[27/50] loss: 0.001123\n",
            "[27/50] loss: 0.000872\n",
            "[27/50] loss: 0.001304\n",
            "[27/50] loss: 0.000654\n",
            "[28/50] loss: 0.001105\n",
            "[28/50] loss: 0.000819\n",
            "[28/50] loss: 0.001271\n",
            "[28/50] loss: 0.000643\n",
            "[29/50] loss: 0.001088\n",
            "[29/50] loss: 0.000771\n",
            "[29/50] loss: 0.001238\n",
            "[29/50] loss: 0.000633\n",
            "[30/50] loss: 0.001072\n",
            "[30/50] loss: 0.000727\n",
            "[30/50] loss: 0.001207\n",
            "[30/50] loss: 0.000623\n",
            "[31/50] loss: 0.001057\n",
            "[31/50] loss: 0.000688\n",
            "[31/50] loss: 0.001176\n",
            "[31/50] loss: 0.000614\n",
            "[32/50] loss: 0.001042\n",
            "[32/50] loss: 0.000653\n",
            "[32/50] loss: 0.001147\n",
            "[32/50] loss: 0.000606\n",
            "[33/50] loss: 0.001028\n",
            "[33/50] loss: 0.000621\n",
            "[33/50] loss: 0.001118\n",
            "[33/50] loss: 0.000598\n",
            "[34/50] loss: 0.001014\n",
            "[34/50] loss: 0.000592\n",
            "[34/50] loss: 0.001091\n",
            "[34/50] loss: 0.000591\n",
            "[35/50] loss: 0.001001\n",
            "[35/50] loss: 0.000565\n",
            "[35/50] loss: 0.001064\n",
            "[35/50] loss: 0.000584\n",
            "[36/50] loss: 0.000989\n",
            "[36/50] loss: 0.000541\n",
            "[36/50] loss: 0.001038\n",
            "[36/50] loss: 0.000577\n",
            "[37/50] loss: 0.000977\n",
            "[37/50] loss: 0.000519\n",
            "[37/50] loss: 0.001014\n",
            "[37/50] loss: 0.000571\n",
            "[38/50] loss: 0.000965\n",
            "[38/50] loss: 0.000500\n",
            "[38/50] loss: 0.000990\n",
            "[38/50] loss: 0.000564\n",
            "[39/50] loss: 0.000954\n",
            "[39/50] loss: 0.000481\n",
            "[39/50] loss: 0.000967\n",
            "[39/50] loss: 0.000559\n",
            "[40/50] loss: 0.000943\n",
            "[40/50] loss: 0.000465\n",
            "[40/50] loss: 0.000945\n",
            "[40/50] loss: 0.000553\n",
            "[41/50] loss: 0.000932\n",
            "[41/50] loss: 0.000450\n",
            "[41/50] loss: 0.000924\n",
            "[41/50] loss: 0.000548\n",
            "[42/50] loss: 0.000922\n",
            "[42/50] loss: 0.000436\n",
            "[42/50] loss: 0.000903\n",
            "[42/50] loss: 0.000543\n",
            "[43/50] loss: 0.000912\n",
            "[43/50] loss: 0.000423\n",
            "[43/50] loss: 0.000884\n",
            "[43/50] loss: 0.000538\n",
            "[44/50] loss: 0.000902\n",
            "[44/50] loss: 0.000412\n",
            "[44/50] loss: 0.000865\n",
            "[44/50] loss: 0.000533\n",
            "[45/50] loss: 0.000892\n",
            "[45/50] loss: 0.000401\n",
            "[45/50] loss: 0.000846\n",
            "[45/50] loss: 0.000529\n",
            "[46/50] loss: 0.000883\n",
            "[46/50] loss: 0.000392\n",
            "[46/50] loss: 0.000829\n",
            "[46/50] loss: 0.000524\n",
            "[47/50] loss: 0.000874\n",
            "[47/50] loss: 0.000383\n",
            "[47/50] loss: 0.000812\n",
            "[47/50] loss: 0.000520\n",
            "[48/50] loss: 0.000865\n",
            "[48/50] loss: 0.000375\n",
            "[48/50] loss: 0.000796\n",
            "[48/50] loss: 0.000516\n",
            "[49/50] loss: 0.000856\n",
            "[49/50] loss: 0.000367\n",
            "[49/50] loss: 0.000780\n",
            "[49/50] loss: 0.000512\n",
            "[50/50] loss: 0.000847\n",
            "[50/50] loss: 0.000360\n",
            "[50/50] loss: 0.000764\n",
            "[50/50] loss: 0.000508\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss = []\n",
        "\n",
        "nepoch = 50\n",
        "for epoch in range(nepoch):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        \n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "  \n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "        if i % 100 == 1:    # print every 200 mini-batches\n",
        "            print(f'[{epoch + 1}/{nepoch}] loss: {running_loss / 100:.6f}')\n",
        "            running_loss = 0.0\n",
        "    test_loss.append(running_loss)\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxLJsDmk9LZg",
        "outputId": "cf0bbdba-744e-4bc5-c80e-ac918a87457c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:520: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([1, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50] loss: 0.000035\n",
            "[1/50] loss: 0.068737\n",
            "[1/50] loss: 0.064776\n",
            "[1/50] loss: 0.059429\n",
            "[1/50] loss: 0.042080\n",
            "[1/50] loss: 0.044232\n",
            "[1/50] loss: 0.053870\n",
            "[1/50] loss: 0.038419\n",
            "[1/50] loss: 0.039553\n",
            "[1/50] loss: 0.040133\n",
            "[2/50] loss: 0.000048\n",
            "[2/50] loss: 0.032694\n",
            "[2/50] loss: 0.035208\n",
            "[2/50] loss: 0.040034\n",
            "[2/50] loss: 0.028299\n",
            "[2/50] loss: 0.028393\n",
            "[2/50] loss: 0.035056\n",
            "[2/50] loss: 0.024530\n",
            "[2/50] loss: 0.026514\n",
            "[2/50] loss: 0.027376\n",
            "[3/50] loss: 0.000042\n",
            "[3/50] loss: 0.026122\n",
            "[3/50] loss: 0.026962\n",
            "[3/50] loss: 0.034948\n",
            "[3/50] loss: 0.023606\n",
            "[3/50] loss: 0.023469\n",
            "[3/50] loss: 0.028380\n",
            "[3/50] loss: 0.020053\n",
            "[3/50] loss: 0.021319\n",
            "[3/50] loss: 0.022090\n",
            "[4/50] loss: 0.000034\n",
            "[4/50] loss: 0.022438\n",
            "[4/50] loss: 0.022562\n",
            "[4/50] loss: 0.031919\n",
            "[4/50] loss: 0.020383\n",
            "[4/50] loss: 0.020453\n",
            "[4/50] loss: 0.024263\n",
            "[4/50] loss: 0.017365\n",
            "[4/50] loss: 0.018051\n",
            "[4/50] loss: 0.018775\n",
            "[5/50] loss: 0.000028\n",
            "[5/50] loss: 0.019518\n",
            "[5/50] loss: 0.019430\n",
            "[5/50] loss: 0.029465\n",
            "[5/50] loss: 0.017739\n",
            "[5/50] loss: 0.018051\n",
            "[5/50] loss: 0.021097\n",
            "[5/50] loss: 0.015271\n",
            "[5/50] loss: 0.015579\n",
            "[5/50] loss: 0.016280\n",
            "[6/50] loss: 0.000023\n",
            "[6/50] loss: 0.017049\n",
            "[6/50] loss: 0.016949\n",
            "[6/50] loss: 0.027354\n",
            "[6/50] loss: 0.015510\n",
            "[6/50] loss: 0.016020\n",
            "[6/50] loss: 0.018497\n",
            "[6/50] loss: 0.013527\n",
            "[6/50] loss: 0.013593\n",
            "[6/50] loss: 0.014266\n",
            "[7/50] loss: 0.000020\n",
            "[7/50] loss: 0.014945\n",
            "[7/50] loss: 0.014904\n",
            "[7/50] loss: 0.025527\n",
            "[7/50] loss: 0.013624\n",
            "[7/50] loss: 0.014288\n",
            "[7/50] loss: 0.016316\n",
            "[7/50] loss: 0.012048\n",
            "[7/50] loss: 0.011960\n",
            "[7/50] loss: 0.012591\n",
            "[8/50] loss: 0.000018\n",
            "[8/50] loss: 0.013148\n",
            "[8/50] loss: 0.013188\n",
            "[8/50] loss: 0.023943\n",
            "[8/50] loss: 0.012024\n",
            "[8/50] loss: 0.012804\n",
            "[8/50] loss: 0.014468\n",
            "[8/50] loss: 0.010787\n",
            "[8/50] loss: 0.010599\n",
            "[8/50] loss: 0.011178\n",
            "[9/50] loss: 0.000017\n",
            "[9/50] loss: 0.011615\n",
            "[9/50] loss: 0.011734\n",
            "[9/50] loss: 0.022564\n",
            "[9/50] loss: 0.010663\n",
            "[9/50] loss: 0.011525\n",
            "[9/50] loss: 0.012892\n",
            "[9/50] loss: 0.009705\n",
            "[9/50] loss: 0.009458\n",
            "[9/50] loss: 0.009976\n",
            "[10/50] loss: 0.000016\n",
            "[10/50] loss: 0.010307\n",
            "[10/50] loss: 0.010493\n",
            "[10/50] loss: 0.021360\n",
            "[10/50] loss: 0.009502\n",
            "[10/50] loss: 0.010423\n",
            "[10/50] loss: 0.011544\n",
            "[10/50] loss: 0.008775\n",
            "[10/50] loss: 0.008495\n",
            "[10/50] loss: 0.008948\n",
            "[11/50] loss: 0.000015\n",
            "[11/50] loss: 0.009190\n",
            "[11/50] loss: 0.009428\n",
            "[11/50] loss: 0.020306\n",
            "[11/50] loss: 0.008508\n",
            "[11/50] loss: 0.009468\n",
            "[11/50] loss: 0.010386\n",
            "[11/50] loss: 0.007971\n",
            "[11/50] loss: 0.007675\n",
            "[11/50] loss: 0.008064\n",
            "[12/50] loss: 0.000014\n",
            "[12/50] loss: 0.008231\n",
            "[12/50] loss: 0.008511\n",
            "[12/50] loss: 0.019376\n",
            "[12/50] loss: 0.007655\n",
            "[12/50] loss: 0.008637\n",
            "[12/50] loss: 0.009386\n",
            "[12/50] loss: 0.007273\n",
            "[12/50] loss: 0.006974\n",
            "[12/50] loss: 0.007302\n",
            "[13/50] loss: 0.000014\n",
            "[13/50] loss: 0.007408\n",
            "[13/50] loss: 0.007717\n",
            "[13/50] loss: 0.018553\n",
            "[13/50] loss: 0.006920\n",
            "[13/50] loss: 0.007912\n",
            "[13/50] loss: 0.008520\n",
            "[13/50] loss: 0.006665\n",
            "[13/50] loss: 0.006372\n",
            "[13/50] loss: 0.006641\n",
            "[14/50] loss: 0.000014\n",
            "[14/50] loss: 0.006698\n",
            "[14/50] loss: 0.007028\n",
            "[14/50] loss: 0.017819\n",
            "[14/50] loss: 0.006285\n",
            "[14/50] loss: 0.007275\n",
            "[14/50] loss: 0.007768\n",
            "[14/50] loss: 0.006133\n",
            "[14/50] loss: 0.005852\n",
            "[14/50] loss: 0.006068\n",
            "[15/50] loss: 0.000014\n",
            "[15/50] loss: 0.006085\n",
            "[15/50] loss: 0.006427\n",
            "[15/50] loss: 0.017164\n",
            "[15/50] loss: 0.005736\n",
            "[15/50] loss: 0.006715\n",
            "[15/50] loss: 0.007113\n",
            "[15/50] loss: 0.005666\n",
            "[15/50] loss: 0.005402\n",
            "[15/50] loss: 0.005568\n",
            "[16/50] loss: 0.000014\n",
            "[16/50] loss: 0.005555\n",
            "[16/50] loss: 0.005903\n",
            "[16/50] loss: 0.016576\n",
            "[16/50] loss: 0.005259\n",
            "[16/50] loss: 0.006220\n",
            "[16/50] loss: 0.006539\n",
            "[16/50] loss: 0.005254\n",
            "[16/50] loss: 0.005009\n",
            "[16/50] loss: 0.005130\n",
            "[17/50] loss: 0.000014\n",
            "[17/50] loss: 0.005095\n",
            "[17/50] loss: 0.005441\n",
            "[17/50] loss: 0.016044\n",
            "[17/50] loss: 0.004842\n",
            "[17/50] loss: 0.005780\n",
            "[17/50] loss: 0.006035\n",
            "[17/50] loss: 0.004890\n",
            "[17/50] loss: 0.004665\n",
            "[17/50] loss: 0.004745\n",
            "[18/50] loss: 0.000014\n",
            "[18/50] loss: 0.004693\n",
            "[18/50] loss: 0.005035\n",
            "[18/50] loss: 0.015563\n",
            "[18/50] loss: 0.004477\n",
            "[18/50] loss: 0.005389\n",
            "[18/50] loss: 0.005591\n",
            "[18/50] loss: 0.004566\n",
            "[18/50] loss: 0.004363\n",
            "[18/50] loss: 0.004406\n",
            "[19/50] loss: 0.000015\n",
            "[19/50] loss: 0.004342\n",
            "[19/50] loss: 0.004677\n",
            "[19/50] loss: 0.015125\n",
            "[19/50] loss: 0.004158\n",
            "[19/50] loss: 0.005040\n",
            "[19/50] loss: 0.005198\n",
            "[19/50] loss: 0.004277\n",
            "[19/50] loss: 0.004095\n",
            "[19/50] loss: 0.004107\n",
            "[20/50] loss: 0.000015\n",
            "[20/50] loss: 0.004034\n",
            "[20/50] loss: 0.004360\n",
            "[20/50] loss: 0.014725\n",
            "[20/50] loss: 0.003877\n",
            "[20/50] loss: 0.004727\n",
            "[20/50] loss: 0.004851\n",
            "[20/50] loss: 0.004018\n",
            "[20/50] loss: 0.003856\n",
            "[20/50] loss: 0.003841\n",
            "[21/50] loss: 0.000016\n",
            "[21/50] loss: 0.003764\n",
            "[21/50] loss: 0.004079\n",
            "[21/50] loss: 0.014359\n",
            "[21/50] loss: 0.003628\n",
            "[21/50] loss: 0.004446\n",
            "[21/50] loss: 0.004542\n",
            "[21/50] loss: 0.003786\n",
            "[21/50] loss: 0.003644\n",
            "[21/50] loss: 0.003605\n",
            "[22/50] loss: 0.000016\n",
            "[22/50] loss: 0.003527\n",
            "[22/50] loss: 0.003829\n",
            "[22/50] loss: 0.014022\n",
            "[22/50] loss: 0.003409\n",
            "[22/50] loss: 0.004194\n",
            "[22/50] loss: 0.004267\n",
            "[22/50] loss: 0.003576\n",
            "[22/50] loss: 0.003454\n",
            "[22/50] loss: 0.003393\n",
            "[23/50] loss: 0.000017\n",
            "[23/50] loss: 0.003317\n",
            "[23/50] loss: 0.003605\n",
            "[23/50] loss: 0.013713\n",
            "[23/50] loss: 0.003214\n",
            "[23/50] loss: 0.003965\n",
            "[23/50] loss: 0.004021\n",
            "[23/50] loss: 0.003388\n",
            "[23/50] loss: 0.003284\n",
            "[23/50] loss: 0.003205\n",
            "[24/50] loss: 0.000018\n",
            "[24/50] loss: 0.003132\n",
            "[24/50] loss: 0.003404\n",
            "[24/50] loss: 0.013426\n",
            "[24/50] loss: 0.003042\n",
            "[24/50] loss: 0.003758\n",
            "[24/50] loss: 0.003800\n",
            "[24/50] loss: 0.003217\n",
            "[24/50] loss: 0.003130\n",
            "[24/50] loss: 0.003035\n",
            "[25/50] loss: 0.000019\n",
            "[25/50] loss: 0.002968\n",
            "[25/50] loss: 0.003224\n",
            "[25/50] loss: 0.013161\n",
            "[25/50] loss: 0.002887\n",
            "[25/50] loss: 0.003570\n",
            "[25/50] loss: 0.003601\n",
            "[25/50] loss: 0.003061\n",
            "[25/50] loss: 0.002990\n",
            "[25/50] loss: 0.002882\n",
            "[26/50] loss: 0.000020\n",
            "[26/50] loss: 0.002821\n",
            "[26/50] loss: 0.003061\n",
            "[26/50] loss: 0.012915\n",
            "[26/50] loss: 0.002750\n",
            "[26/50] loss: 0.003398\n",
            "[26/50] loss: 0.003422\n",
            "[26/50] loss: 0.002920\n",
            "[26/50] loss: 0.002864\n",
            "[26/50] loss: 0.002743\n",
            "[27/50] loss: 0.000021\n",
            "[27/50] loss: 0.002690\n",
            "[27/50] loss: 0.002913\n",
            "[27/50] loss: 0.012685\n",
            "[27/50] loss: 0.002625\n",
            "[27/50] loss: 0.003241\n",
            "[27/50] loss: 0.003259\n",
            "[27/50] loss: 0.002790\n",
            "[27/50] loss: 0.002748\n",
            "[27/50] loss: 0.002617\n",
            "[28/50] loss: 0.000022\n",
            "[28/50] loss: 0.002573\n",
            "[28/50] loss: 0.002779\n",
            "[28/50] loss: 0.012473\n",
            "[28/50] loss: 0.002513\n",
            "[28/50] loss: 0.003098\n",
            "[28/50] loss: 0.003111\n",
            "[28/50] loss: 0.002672\n",
            "[28/50] loss: 0.002643\n",
            "[28/50] loss: 0.002502\n",
            "[29/50] loss: 0.000023\n",
            "[29/50] loss: 0.002468\n",
            "[29/50] loss: 0.002658\n",
            "[29/50] loss: 0.012272\n",
            "[29/50] loss: 0.002412\n",
            "[29/50] loss: 0.002967\n",
            "[29/50] loss: 0.002976\n",
            "[29/50] loss: 0.002562\n",
            "[29/50] loss: 0.002545\n",
            "[29/50] loss: 0.002398\n",
            "[30/50] loss: 0.000025\n",
            "[30/50] loss: 0.002373\n",
            "[30/50] loss: 0.002547\n",
            "[30/50] loss: 0.012085\n",
            "[30/50] loss: 0.002320\n",
            "[30/50] loss: 0.002846\n",
            "[30/50] loss: 0.002853\n",
            "[30/50] loss: 0.002461\n",
            "[30/50] loss: 0.002456\n",
            "[30/50] loss: 0.002302\n",
            "[31/50] loss: 0.000026\n",
            "[31/50] loss: 0.002287\n",
            "[31/50] loss: 0.002445\n",
            "[31/50] loss: 0.011909\n",
            "[31/50] loss: 0.002236\n",
            "[31/50] loss: 0.002734\n",
            "[31/50] loss: 0.002740\n",
            "[31/50] loss: 0.002368\n",
            "[31/50] loss: 0.002373\n",
            "[31/50] loss: 0.002215\n",
            "[32/50] loss: 0.000027\n",
            "[32/50] loss: 0.002209\n",
            "[32/50] loss: 0.002352\n",
            "[32/50] loss: 0.011745\n",
            "[32/50] loss: 0.002159\n",
            "[32/50] loss: 0.002630\n",
            "[32/50] loss: 0.002637\n",
            "[32/50] loss: 0.002282\n",
            "[32/50] loss: 0.002297\n",
            "[32/50] loss: 0.002134\n",
            "[33/50] loss: 0.000028\n",
            "[33/50] loss: 0.002138\n",
            "[33/50] loss: 0.002266\n",
            "[33/50] loss: 0.011590\n",
            "[33/50] loss: 0.002089\n",
            "[33/50] loss: 0.002534\n",
            "[33/50] loss: 0.002542\n",
            "[33/50] loss: 0.002202\n",
            "[33/50] loss: 0.002225\n",
            "[33/50] loss: 0.002060\n",
            "[34/50] loss: 0.000030\n",
            "[34/50] loss: 0.002072\n",
            "[34/50] loss: 0.002186\n",
            "[34/50] loss: 0.011444\n",
            "[34/50] loss: 0.002024\n",
            "[34/50] loss: 0.002444\n",
            "[34/50] loss: 0.002453\n",
            "[34/50] loss: 0.002127\n",
            "[34/50] loss: 0.002159\n",
            "[34/50] loss: 0.001991\n",
            "[35/50] loss: 0.000031\n",
            "[35/50] loss: 0.002013\n",
            "[35/50] loss: 0.002113\n",
            "[35/50] loss: 0.011305\n",
            "[35/50] loss: 0.001965\n",
            "[35/50] loss: 0.002361\n",
            "[35/50] loss: 0.002372\n",
            "[35/50] loss: 0.002057\n",
            "[35/50] loss: 0.002097\n",
            "[35/50] loss: 0.001928\n",
            "[36/50] loss: 0.000032\n",
            "[36/50] loss: 0.001958\n",
            "[36/50] loss: 0.002045\n",
            "[36/50] loss: 0.011175\n",
            "[36/50] loss: 0.001909\n",
            "[36/50] loss: 0.002282\n",
            "[36/50] loss: 0.002296\n",
            "[36/50] loss: 0.001991\n",
            "[36/50] loss: 0.002039\n",
            "[36/50] loss: 0.001869\n",
            "[37/50] loss: 0.000034\n",
            "[37/50] loss: 0.001908\n",
            "[37/50] loss: 0.001982\n",
            "[37/50] loss: 0.011051\n",
            "[37/50] loss: 0.001858\n",
            "[37/50] loss: 0.002209\n",
            "[37/50] loss: 0.002225\n",
            "[37/50] loss: 0.001930\n",
            "[37/50] loss: 0.001984\n",
            "[37/50] loss: 0.001814\n",
            "[38/50] loss: 0.000035\n",
            "[38/50] loss: 0.001861\n",
            "[38/50] loss: 0.001923\n",
            "[38/50] loss: 0.010934\n",
            "[38/50] loss: 0.001811\n",
            "[38/50] loss: 0.002141\n",
            "[38/50] loss: 0.002160\n",
            "[38/50] loss: 0.001872\n",
            "[38/50] loss: 0.001933\n",
            "[38/50] loss: 0.001763\n",
            "[39/50] loss: 0.000036\n",
            "[39/50] loss: 0.001818\n",
            "[39/50] loss: 0.001869\n",
            "[39/50] loss: 0.010822\n",
            "[39/50] loss: 0.001767\n",
            "[39/50] loss: 0.002077\n",
            "[39/50] loss: 0.002098\n",
            "[39/50] loss: 0.001818\n",
            "[39/50] loss: 0.001885\n",
            "[39/50] loss: 0.001715\n",
            "[40/50] loss: 0.000038\n",
            "[40/50] loss: 0.001778\n",
            "[40/50] loss: 0.001818\n",
            "[40/50] loss: 0.010717\n",
            "[40/50] loss: 0.001726\n",
            "[40/50] loss: 0.002017\n",
            "[40/50] loss: 0.002041\n",
            "[40/50] loss: 0.001767\n",
            "[40/50] loss: 0.001839\n",
            "[40/50] loss: 0.001670\n",
            "[41/50] loss: 0.000039\n",
            "[41/50] loss: 0.001742\n",
            "[41/50] loss: 0.001770\n",
            "[41/50] loss: 0.010616\n",
            "[41/50] loss: 0.001688\n",
            "[41/50] loss: 0.001960\n",
            "[41/50] loss: 0.001987\n",
            "[41/50] loss: 0.001719\n",
            "[41/50] loss: 0.001796\n",
            "[41/50] loss: 0.001628\n",
            "[42/50] loss: 0.000040\n",
            "[42/50] loss: 0.001707\n",
            "[42/50] loss: 0.001725\n",
            "[42/50] loss: 0.010520\n",
            "[42/50] loss: 0.001652\n",
            "[42/50] loss: 0.001907\n",
            "[42/50] loss: 0.001936\n",
            "[42/50] loss: 0.001673\n",
            "[42/50] loss: 0.001755\n",
            "[42/50] loss: 0.001589\n",
            "[43/50] loss: 0.000042\n",
            "[43/50] loss: 0.001675\n",
            "[43/50] loss: 0.001683\n",
            "[43/50] loss: 0.010429\n",
            "[43/50] loss: 0.001619\n",
            "[43/50] loss: 0.001857\n",
            "[43/50] loss: 0.001888\n",
            "[43/50] loss: 0.001631\n",
            "[43/50] loss: 0.001717\n",
            "[43/50] loss: 0.001552\n",
            "[44/50] loss: 0.000043\n",
            "[44/50] loss: 0.001645\n",
            "[44/50] loss: 0.001643\n",
            "[44/50] loss: 0.010342\n",
            "[44/50] loss: 0.001587\n",
            "[44/50] loss: 0.001810\n",
            "[44/50] loss: 0.001844\n",
            "[44/50] loss: 0.001590\n",
            "[44/50] loss: 0.001680\n",
            "[44/50] loss: 0.001516\n",
            "[45/50] loss: 0.000044\n",
            "[45/50] loss: 0.001617\n",
            "[45/50] loss: 0.001605\n",
            "[45/50] loss: 0.010259\n",
            "[45/50] loss: 0.001557\n",
            "[45/50] loss: 0.001765\n",
            "[45/50] loss: 0.001801\n",
            "[45/50] loss: 0.001552\n",
            "[45/50] loss: 0.001645\n",
            "[45/50] loss: 0.001483\n",
            "[46/50] loss: 0.000046\n",
            "[46/50] loss: 0.001591\n",
            "[46/50] loss: 0.001570\n",
            "[46/50] loss: 0.010179\n",
            "[46/50] loss: 0.001529\n",
            "[46/50] loss: 0.001722\n",
            "[46/50] loss: 0.001761\n",
            "[46/50] loss: 0.001515\n",
            "[46/50] loss: 0.001611\n",
            "[46/50] loss: 0.001451\n",
            "[47/50] loss: 0.000047\n",
            "[47/50] loss: 0.001566\n",
            "[47/50] loss: 0.001536\n",
            "[47/50] loss: 0.010103\n",
            "[47/50] loss: 0.001503\n",
            "[47/50] loss: 0.001682\n",
            "[47/50] loss: 0.001723\n",
            "[47/50] loss: 0.001481\n",
            "[47/50] loss: 0.001579\n",
            "[47/50] loss: 0.001421\n",
            "[48/50] loss: 0.000048\n",
            "[48/50] loss: 0.001543\n",
            "[48/50] loss: 0.001505\n",
            "[48/50] loss: 0.010029\n",
            "[48/50] loss: 0.001478\n",
            "[48/50] loss: 0.001643\n",
            "[48/50] loss: 0.001687\n",
            "[48/50] loss: 0.001448\n",
            "[48/50] loss: 0.001549\n",
            "[48/50] loss: 0.001393\n",
            "[49/50] loss: 0.000049\n",
            "[49/50] loss: 0.001521\n",
            "[49/50] loss: 0.001475\n",
            "[49/50] loss: 0.009959\n",
            "[49/50] loss: 0.001455\n",
            "[49/50] loss: 0.001607\n",
            "[49/50] loss: 0.001652\n",
            "[49/50] loss: 0.001417\n",
            "[49/50] loss: 0.001520\n",
            "[49/50] loss: 0.001366\n",
            "[50/50] loss: 0.000050\n",
            "[50/50] loss: 0.001500\n",
            "[50/50] loss: 0.001446\n",
            "[50/50] loss: 0.009892\n",
            "[50/50] loss: 0.001432\n",
            "[50/50] loss: 0.001573\n",
            "[50/50] loss: 0.001620\n",
            "[50/50] loss: 0.001388\n",
            "[50/50] loss: 0.001492\n",
            "[50/50] loss: 0.001341\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGS3x7yo_DlP",
        "outputId": "aea96c67-5f9a-4c83-e2ea-5381286dbc29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "plt.plot(range(nepoch),train_loss,label='train_loss')\n",
        "plt.plot(range(nepoch),test_loss,label='test_loss')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "XvV9QHMa4Cuf",
        "outputId": "af962048-4624-4901-e01d-e8d8fbd85149"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f0d2b978c50>]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jOAm_VDk8vhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        # print((predicted,labels))\n",
        "\n",
        "print(f'Accuracy of the network on the test sample: {100 * correct / total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5QpqsLi18J_",
        "outputId": "c48cfd64-44e1-4b61-f366-5113cd3f33b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test sample: 57.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Wl8Pc9rO2fvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}